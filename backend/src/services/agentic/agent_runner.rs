//! AI-Driven Narrative Agent Runner V3
//!
//! This is a complete rewrite of the agent runner following the Living World Implementation
//! Roadmap and Hierarchical Agent Framework architecture. This implementation:
//!
//! - Uses Flash/Flash-Lite abstraction for all AI calls (Epic 1 requirement)
//! - Implements proper AI-driven triage and planning (vs hardcoded logic)
//! - Follows the Prompt Orchestration Engine philosophy
//! - Prepares foundation for Tactical Layer integration (Epic 4)
//! - Maintains security-first design with SessionDek integration
//!
//! Key improvements over V2:
//! - All AI calls go through Flash abstraction layer
//! - AI-powered triage analysis replaces hardcoded rules
//! - AI-powered action planning replaces massive hardcoded prompts
//! - Clean separation of concerns for hierarchical agent evolution
//! - Comprehensive error handling and logging

use serde_json::{json, Value};
use std::sync::Arc;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use chrono;
use crate::{
    auth::session_dek::SessionDek,
    errors::AppError,
    models::{
        chats::ChatMessage,
        chronicle::CreateChronicleRequest,
        chronicle_event::EventFilter,
    },
    services::{ChronicleService, hybrid_token_counter::HybridTokenCounter},
    state::AppState,
};

use super::{
    registry::ToolRegistry,
    tools::{ToolParams, ToolResult, ai_narrative_analysis::{AiTriageAnalyzer, AiPlanGenerator}},
};

/// Configuration for the AI-driven narrative workflow
#[derive(Debug, Clone)]
pub struct NarrativeWorkflowConfig {
    /// Model to use for triage analysis (should be fast and cheap)
    pub triage_model: String,
    /// Model to use for reasoning and planning (can be more capable)
    pub planning_model: String,
    /// Maximum number of tools that can be executed in a single workflow
    pub max_tool_executions: usize,
    /// Whether to enable cost-saving optimizations
    pub enable_cost_optimizations: bool,
}

impl Default for NarrativeWorkflowConfig {
    fn default() -> Self {
        Self {
            triage_model: "gemini-2.5-flash-lite-preview-06-17".to_string(),
            planning_model: "gemini-2.5-flash-lite-preview-06-17".to_string(),
            max_tool_executions: 5,
            enable_cost_optimizations: true,
        }
    }
}

/// Result of the AI-driven triage analysis step
#[derive(Debug, Clone)]
pub struct TriageResult {
    pub is_significant: bool,
    pub summary: String,
    pub event_type: String,
    pub confidence: f32,
    pub reasoning: String,
}

/// A plan of action generated by the AI agent
#[derive(Debug, Clone)]
pub struct ActionPlan {
    pub reasoning: String,
    pub actions: Vec<PlannedAction>,
    pub confidence: f32,
}

/// A single action in the plan
#[derive(Debug, Clone)]
pub struct PlannedAction {
    pub tool_name: String,
    pub parameters: ToolParams,
    pub reasoning: String,
}

/// User persona context for AI analysis
#[derive(Debug, Clone)]
pub struct UserPersonaContext {
    pub persona_name: String,
    pub persona_description: String,
    pub character_traits: Vec<String>,
}

impl UserPersonaContext {
    pub fn to_prompt_context(&self) -> String {
        format!(
            "PERSONA CONTEXT:\n- Name: {}\n- Description: {}\n- Traits: {}",
            self.persona_name,
            self.persona_description,
            self.character_traits.join(", ")
        )
    }
}

/// AI-driven agent runner that orchestrates intelligent narrative workflows
/// 
/// This implementation follows the Hierarchical Agent Framework principles:
/// - Prepares foundation for Tactical Layer integration
/// - Uses AI-driven decision making instead of hardcoded rules
/// - Maintains clean Flash abstraction for all AI calls
/// - Implements proper security and error handling
pub struct NarrativeAgentRunner {
    app_state: Arc<AppState>,
    tool_registry: Arc<ToolRegistry>,
    config: NarrativeWorkflowConfig,
    chronicle_service: Arc<ChronicleService>,
    _token_counter: Arc<HybridTokenCounter>, // TODO: Implement token tracking
    // AI-powered components for intelligent analysis
    triage_analyzer: Arc<AiTriageAnalyzer>,
    plan_generator: Arc<AiPlanGenerator>,
}

impl NarrativeAgentRunner {
    pub fn new(
        app_state: Arc<AppState>,
        tool_registry: Arc<ToolRegistry>,
        config: NarrativeWorkflowConfig,
        chronicle_service: Arc<ChronicleService>,
        token_counter: Arc<HybridTokenCounter>,
    ) -> Self {
        // Create AI-powered components
        let triage_analyzer = Arc::new(AiTriageAnalyzer::new(app_state.clone()));
        let plan_generator = Arc::new(AiPlanGenerator::new(app_state.clone()));

        Self {
            app_state,
            tool_registry,
            config,
            chronicle_service,
            _token_counter: token_counter,
            triage_analyzer,
            plan_generator,
        }
    }

    /// Main entry point for processing narrative content
    /// 
    /// This method orchestrates the full AI-driven workflow:
    /// 1. AI-powered triage analysis to determine significance
    /// 2. AI-powered action planning based on triage results
    /// 3. Execution of planned actions with error handling
    /// 4. Chronicle creation if significant events occurred
    pub async fn process_narrative_content(
        &self,
        messages: &[ChatMessage],
        session_dek: &SessionDek,
        user_id: Uuid,
        chronicle_id: Option<Uuid>,
        persona_context: Option<&UserPersonaContext>,
        is_re_chronicle: bool,
        context: &str,
    ) -> Result<Value, AppError> {
        info!("Starting AI-driven narrative processing for {} messages", messages.len());

        // Step 1: AI-powered triage analysis
        let triage_result = self.perform_ai_triage(
            messages,
            session_dek,
            user_id,
            chronicle_id,
            persona_context,
            is_re_chronicle,
            context,
        ).await?;

        debug!("Triage completed: significant={}, confidence={}", 
               triage_result.is_significant, triage_result.confidence);

        // Step 2: Handle significant events with AI-powered planning
        if triage_result.is_significant || is_re_chronicle {
            let chronicle_id = self.ensure_chronicle_exists(
                chronicle_id,
                user_id,
                &triage_result,
            ).await?;

            // Step 3: Gather knowledge context for intelligent planning
            let knowledge_context = self.build_knowledge_context(
                user_id,
                Some(chronicle_id),
                session_dek,
            ).await?;

            // Step 4: AI-powered action planning with full context
            let action_plan = self.generate_ai_action_plan(
                &triage_result,
                &knowledge_context,
                Some(chronicle_id),
                chronicle_id == chronicle_id, // chronicle_was_just_created logic
                persona_context,
                is_re_chronicle,
            ).await?;

            debug!("Generated action plan with {} actions", action_plan.actions.len());

            // Step 4: Execute planned actions
            let execution_results = self.execute_action_plan(&action_plan, session_dek, user_id).await?;

            info!("Narrative processing completed successfully with {} actions executed", 
                  action_plan.actions.len());

            Ok(json!({
                "triage": {
                    "is_significant": triage_result.is_significant,
                    "summary": triage_result.summary,
                    "event_type": triage_result.event_type,
                    "confidence": triage_result.confidence,
                    "reasoning": triage_result.reasoning
                },
                "plan": {
                    "reasoning": action_plan.reasoning,
                    "actions_planned": action_plan.actions.len(),
                    "confidence": action_plan.confidence
                },
                "execution": execution_results,
                "chronicle_id": chronicle_id
            }))
        } else {
            debug!("Event not significant enough for processing (confidence: {})", triage_result.confidence);
            
            Ok(json!({
                "triage": {
                    "is_significant": false,
                    "summary": triage_result.summary,
                    "reasoning": triage_result.reasoning,
                    "confidence": triage_result.confidence
                },
                "action_taken": "none"
            }))
        }
    }

    /// Perform AI-driven triage analysis using Flash-Lite
    /// 
    /// This replaces the old hardcoded triage logic with intelligent AI analysis
    /// that can understand context, narrative significance, and user intent.
    async fn perform_ai_triage(
        &self,
        messages: &[ChatMessage],
        session_dek: &SessionDek,
        user_id: Uuid,
        chronicle_id: Option<Uuid>,
        persona_context: Option<&UserPersonaContext>,
        is_re_chronicle: bool,
        context: &str,
    ) -> Result<TriageResult, AppError> {
        debug!("Starting AI-driven triage analysis");

        // Use the AI triage analyzer component
        let result = self.triage_analyzer.analyze_significance(
            messages,
            session_dek,
            user_id,
            chronicle_id,
            persona_context,
            is_re_chronicle,
            context,
        ).await?;

        info!("AI triage completed: {} (confidence: {})", 
              if result.is_significant { "SIGNIFICANT" } else { "NOT SIGNIFICANT" },
              result.confidence);

        Ok(TriageResult {
            is_significant: result.is_significant,
            summary: result.summary,
            event_type: result.event_type,
            confidence: result.confidence,
            reasoning: "AI-driven analysis using Flash-Lite".to_string(),
        })
    }

    /// Generate AI-driven action plan using Flash
    /// 
    /// This replaces the massive hardcoded prompt logic with clean AI-driven planning
    /// that can adapt to different scenarios and contexts intelligently.
    async fn generate_ai_action_plan(
        &self,
        triage_result: &TriageResult,
        knowledge_context: &Value,
        chronicle_id: Option<Uuid>,
        chronicle_was_just_created: bool,
        persona_context: Option<&UserPersonaContext>,
        is_re_chronicle: bool,
    ) -> Result<ActionPlan, AppError> {
        debug!("Generating AI-driven action plan for event: {}", triage_result.event_type);

        // Get available tools
        let available_tools = self.tool_registry.list_tools();

        // Use AI-powered plan generator
        let result = self.plan_generator.generate_intelligent_plan(
            triage_result,
            knowledge_context,
            chronicle_id,
            chronicle_was_just_created,
            persona_context,
            is_re_chronicle,
            &available_tools,
        ).await?;

        info!("AI action plan generated with {} actions (confidence: {})", 
              result.actions.len(), result.confidence);

        Ok(ActionPlan {
            reasoning: result.reasoning,
            actions: result.actions.into_iter().map(|action| PlannedAction {
                tool_name: action.tool_name,
                parameters: action.parameters,
                reasoning: action.reasoning,
            }).collect(),
            confidence: result.confidence, // Use actual AI-provided confidence
        })
    }

    /// Execute the AI-generated action plan
    /// 
    /// This method executes each planned action in sequence, with proper error
    /// handling and logging for security and debugging purposes.
    async fn execute_action_plan(
        &self,
        action_plan: &ActionPlan,
        session_dek: &SessionDek,
        user_id: Uuid,
    ) -> Result<Value, AppError> {
        debug!("Executing action plan with {} actions", action_plan.actions.len());

        let mut execution_results = Vec::new();
        let mut executed_count = 0;

        for (index, action) in action_plan.actions.iter().enumerate() {
            if executed_count >= self.config.max_tool_executions {
                warn!("Reached maximum tool execution limit ({})", self.config.max_tool_executions);
                break;
            }

            debug!("Executing action {}/{}: {}", index + 1, action_plan.actions.len(), action.tool_name);

            match self.execute_single_action(action, session_dek, user_id).await {
                Ok(result) => {
                    execution_results.push(json!({
                        "action": action.tool_name,
                        "status": "success",
                        "result": result,
                        "reasoning": action.reasoning
                    }));
                    executed_count += 1;
                    info!("Action {} executed successfully", action.tool_name);
                }
                Err(e) => {
                    error!("Action {} failed: {}", action.tool_name, e);
                    execution_results.push(json!({
                        "action": action.tool_name,
                        "status": "failed",
                        "error": e.to_string(),
                        "reasoning": action.reasoning
                    }));
                    // Continue with next action rather than failing entire plan
                }
            }
        }

        Ok(json!({
            "total_actions": action_plan.actions.len(),
            "executed_actions": executed_count,
            "results": execution_results
        }))
    }

    /// Execute a single action from the plan
    async fn execute_single_action(
        &self,
        action: &PlannedAction,
        _session_dek: &SessionDek,
        user_id: Uuid,
    ) -> Result<ToolResult, AppError> {
        // Add user_id to parameters for security
        let mut enhanced_params = action.parameters.clone();
        if let Some(obj) = enhanced_params.as_object_mut() {
            obj.insert("user_id".to_string(), json!(user_id.to_string()));
        }

        // Get the tool and execute it
        match self.tool_registry.get_tool(&action.tool_name) {
            Ok(tool) => {
                let result = tool.execute(&enhanced_params).await
                    .map_err(|e| AppError::InternalServerErrorGeneric(
                        format!("Tool execution failed for {}: {}", action.tool_name, e)
                    ))?;
                Ok(result)
            }
            Err(e) => {
                Err(AppError::BadRequest(format!("Tool '{}' not found: {}", action.tool_name, e)))
            }
        }
    }

    /// Ensure a chronicle exists for significant events
    async fn ensure_chronicle_exists(
        &self,
        chronicle_id: Option<Uuid>,
        user_id: Uuid,
        triage_result: &TriageResult,
    ) -> Result<Uuid, AppError> {
        if let Some(id) = chronicle_id {
            debug!("Using existing chronicle: {}", id);
            Ok(id)
        } else {
            debug!("Creating new chronicle for significant event");
            
            // Generate AI-driven chronicle name
            let chronicle_name = self.generate_ai_chronicle_name(triage_result).await?;
            
            let create_request = CreateChronicleRequest {
                name: chronicle_name,
                description: Some(format!("Chronicle created for: {}", triage_result.summary)),
            };

            let chronicle = self.chronicle_service.create_chronicle(user_id, create_request).await?;
            info!("Created new chronicle: {} ({})", chronicle.name, chronicle.id);
            Ok(chronicle.id)
        }
    }

    /// Build comprehensive knowledge context for AI planning
    /// 
    /// This gathers existing chronicle events, world state, and narrative context
    /// to provide the AI planner with complete information for decision making.
    async fn build_knowledge_context(
        &self,
        user_id: Uuid,
        chronicle_id: Option<Uuid>,
        _session_dek: &SessionDek,
    ) -> Result<Value, AppError> {
        debug!("Building knowledge context for user {} chronicle {:?}", user_id, chronicle_id);

        let mut context = json!({
            "user_id": user_id.to_string(),
            "timestamp": chrono::Utc::now().to_rfc3339()
        });

        // Get existing chronicle events if chronicle exists
        if let Some(c_id) = chronicle_id {
            match self.chronicle_service.get_chronicle_events(user_id, c_id, EventFilter::default()).await {
                Ok(events) => {
                    context["existing_chronicles"] = json!({
                        "chronicle_id": c_id.to_string(),
                        "events": events.into_iter().map(|event| json!({
                            "id": event.id.to_string(),
                            "summary": event.summary,
                            "event_type": event.event_type,
                            "timestamp": event.timestamp_iso8601
                        })).collect::<Vec<_>>()
                    });
                    debug!("Added {} existing chronicle events to context", 
                           context["existing_chronicles"]["events"].as_array().unwrap().len());
                }
                Err(e) => {
                    warn!("Failed to get chronicle events for context: {}", e);
                    context["existing_chronicles"] = json!({
                        "chronicle_id": c_id.to_string(),
                        "events": [],
                        "error": "Failed to retrieve events"
                    });
                }
            }
        } else {
            context["existing_chronicles"] = json!({
                "chronicle_id": null,
                "events": []
            });
        }

        // Add basic world state context
        // TODO: In future iterations, add ECS entity queries, relationships, and spatial context
        context["world_state"] = json!({
            "entities": [],
            "spatial_context": {},
            "note": "ECS integration pending - Epic 2 implementation"
        });

        debug!("Knowledge context built with {} top-level keys", 
               context.as_object().unwrap().len());

        Ok(context)
    }

    /// Generate AI-driven chronicle name using Flash
    /// 
    /// This replaces hardcoded string concatenation with intelligent naming
    /// based on the narrative context and triage results.
    async fn generate_ai_chronicle_name(&self, triage_result: &TriageResult) -> Result<String, AppError> {
        debug!("Generating AI-driven chronicle name");

        let prompt = format!(
            r#"Generate a compelling chronicle name for a narrative event.

EVENT SUMMARY: {}
EVENT TYPE: {}
CONFIDENCE: {}

Generate a creative, memorable chronicle name that captures the essence of this event.
The name should be:
- 3-8 words long
- Evocative and narrative-focused
- Suitable as a story chapter title
- Not too generic or bland

Respond with ONLY the chronicle name, nothing else."#,
            triage_result.summary,
            triage_result.event_type,
            triage_result.confidence
        );

        let chat_request = genai::chat::ChatRequest::from_user(prompt);
        let chat_options = genai::chat::ChatOptions::default()
            .with_max_tokens(50)
            .with_temperature(0.7); // Higher creativity for naming

        let response = self.app_state.ai_client
            .exec_chat(&self.config.planning_model, chat_request, Some(chat_options))
            .await
            .map_err(|e| AppError::LlmClientError(format!("Chronicle name generation failed: {}", e)))?;

        let chronicle_name = response.first_content_text_as_str()
            .unwrap_or("Untitled Chronicle")
            .trim()
            .to_string();

        debug!("Generated chronicle name: {}", chronicle_name);
        Ok(chronicle_name)
    }

    // TEMPORARY STUB METHODS - to be removed during narrative intelligence service refactoring
    // These methods are called by legacy services and will be replaced during Epic 1 Flash integration
    
    #[deprecated(note = "Legacy method - will be removed during narrative intelligence service refactoring")]
    pub async fn process_narrative_event(
        &self,
        _user_id: uuid::Uuid,
        _session_id: uuid::Uuid,
        _chronicle_id: Option<uuid::Uuid>,
        _messages: &[crate::models::chats::ChatMessage],
        _session_dek: &crate::auth::session_dek::SessionDek,
        _persona_context: Option<crate::services::agentic::agent_runner::UserPersonaContext>,
    ) -> Result<serde_json::Value, crate::errors::AppError> {
        // Temporary stub - redirect to new method
        self.process_narrative_content(_messages, _session_dek, _user_id, _chronicle_id, _persona_context.as_ref(), false, "legacy_context").await
    }

    #[deprecated(note = "Legacy method - will be removed during narrative intelligence service refactoring")]
    pub async fn process_narrative_event_with_options(
        &self,
        _messages: &[crate::models::chats::ChatMessage],
        _session_dek: &crate::auth::session_dek::SessionDek,
        _user_id: uuid::Uuid,
        _chronicle_id: Option<uuid::Uuid>,
        _is_re_chronicle: bool,
        _context: &str,
    ) -> Result<serde_json::Value, crate::errors::AppError> {
        // Temporary stub - redirect to new method
        self.process_narrative_content(_messages, _session_dek, _user_id, _chronicle_id, None, _is_re_chronicle, _context).await
    }

    #[deprecated(note = "Legacy method - will be removed during narrative intelligence service refactoring")]
    pub async fn process_narrative_event_dry_run_with_options(
        &self,
        _messages: &[crate::models::chats::ChatMessage],
        _session_dek: &crate::auth::session_dek::SessionDek,
        _user_id: uuid::Uuid,
        _chronicle_id: Option<uuid::Uuid>,
        _is_re_chronicle: bool,
        _context: &str,
    ) -> Result<serde_json::Value, crate::errors::AppError> {
        // Temporary stub for dry run - just return empty result
        Ok(serde_json::json!({
            "dry_run": true,
            "message": "Legacy dry run method - functionality will be replaced during Flash integration"
        }))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_narrative_workflow_config_default() {
        let config = NarrativeWorkflowConfig::default();
        assert_eq!(config.triage_model, "gemini-2.5-flash-lite-preview-06-17");
        assert_eq!(config.planning_model, "gemini-2.5-flash-lite-preview-06-17");
        assert_eq!(config.max_tool_executions, 5);
        assert!(config.enable_cost_optimizations);
    }

    #[test]
    fn test_user_persona_context_prompt() {
        let persona = UserPersonaContext {
            persona_name: "Sol Steele".to_string(),
            persona_description: "A battle-hardened bounty hunter".to_string(),
            character_traits: vec!["brave".to_string(), "cynical".to_string()],
        };

        let prompt = persona.to_prompt_context();
        assert!(prompt.contains("Sol Steele"));
        assert!(prompt.contains("battle-hardened bounty hunter"));
        assert!(prompt.contains("brave, cynical"));
    }

    #[test]
    fn test_triage_result_structure() {
        let result = TriageResult {
            is_significant: true,
            summary: "Test summary".to_string(),
            event_type: "COMBAT".to_string(),
            confidence: 0.8,
            reasoning: "AI analysis".to_string(),
        };

        assert!(result.is_significant);
        assert_eq!(result.confidence, 0.8);
        assert_eq!(result.event_type, "COMBAT");
    }
}